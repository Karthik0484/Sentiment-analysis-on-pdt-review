"""
Sentiment Analysis App
User Interface for Product Review Sentiment Prediction
Optimized for Performance (Caching & Latency Reduction)
"""

import streamlit as st
import pandas as pd
import numpy as np
import pickle
import os
import re
import nltk
import matplotlib.pyplot as plt
import seaborn as sns

import time
import io
from datetime import datetime
from reportlab.lib import colors
from reportlab.lib.pagesizes import A4
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image, PageBreak
from reportlab.pdfgen import canvas

# ============================================================================
# PERFORMANCE OPTIMIZATION: 1. SETUP & CONFIGURATION
# ============================================================================
st.set_page_config(
    page_title="Sentiment Analysis System",
    page_icon="ü§ñ",
    layout="centered"
)

# ============================================================================
# PERFORMANCE OPTIMIZATION: 2. CACHED RESOURCE LOADING
# ============================================================================

# Define paths
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(BASE_DIR)
MODELS_DIR = os.path.join(PROJECT_ROOT, 'models')

# ============================================================================
# PERFORMANCE OPTIMIZATION: PDF REPORT GENERATION (Refined Layout)
# ============================================================================

class PDFReport:
    def __init__(self, product_name):
        self.product_name = product_name if product_name else "Not Specified"

    def add_page_number(self, canvas, doc):
        """Add footer with page number and system info"""
        canvas.saveState()
        canvas.setFont('Helvetica-Oblique', 8)
        canvas.setStrokeColor(colors.lightgrey)
        canvas.line(1 * inch, 0.75 * inch, 7.5 * inch, 0.75 * inch)
        
        footer_text = "Generated by Sentiment Analysis System | Page %d" % doc.page
        canvas.drawCentredString(A4[0]/2.0, 0.5 * inch, footer_text)
        canvas.restoreState()

def generate_pdf_report(df, product_name, counts, pie_fig, bar_fig):
    """
    Generate a professional PDF report using ReportLab Platypus.
    """
    buffer = io.BytesIO()
    doc = SimpleDocTemplate(
        buffer, 
        pagesize=A4,
        rightMargin=72, leftMargin=72,
        topMargin=72, bottomMargin=72
    )
    
    styles = getSampleStyleSheet()
    
    # Custom Styles
    title_style = ParagraphStyle(
        'MainTitle',
        parent=styles['Heading1'],
        fontSize=28,
        alignment=1, # Center
        spaceAfter=30,
        fontName='Helvetica-Bold',
        textColor=colors.HexColor("#1A1A1A")
    )
    
    section_style = ParagraphStyle(
        'SectionHeader',
        parent=styles['Heading2'],
        fontSize=20,
        fontName='Helvetica-Bold',
        spaceBefore=20,
        spaceAfter=15,
        textColor=colors.HexColor("#2C3E50"),
        borderPadding=(0, 0, 5, 0),
        borderWidth=0,
        borderStyle=None
    )
    
    bullet_style = ParagraphStyle(
        'BulletPoint',
        parent=styles['Normal'],
        fontSize=12,
        leading=18,
        leftIndent=20,
        bulletIndent=10,
        spaceBefore=8
    )

    meta_style = ParagraphStyle(
        'Metadata',
        parent=styles['Normal'],
        fontSize=12,
        leading=16,
        spaceAfter=6
    )

    story = []
    
    # 1. Main Title
    story.append(Paragraph("Sentiment Analysis Report", title_style))
    
    # Draw a line below the title
    story.append(Spacer(1, 5))
    
    # 2. Metadata Section
    curr_time = datetime.now().strftime('%B %d, %Y | %H:%M:%S')
    story.append(Paragraph(f"<b>Product Name:</b> {product_name if product_name else 'Not Specified'}", meta_style))
    story.append(Paragraph(f"<b>Analysis Date:</b> {curr_time}", meta_style))
    story.append(Paragraph(f"<b>Total Reviews Processed:</b> {len(df)}", meta_style))
    story.append(Spacer(1, 15))
    
    # 3. Executive Summary
    story.append(Paragraph("Executive Summary", section_style))
    total = len(df)
    for cat in ['positive', 'neutral', 'negative']:
        count = counts.get(cat, 0)
        perc = (count / total * 100) if total > 0 else 0
        story.append(Paragraph(f"&bull; <b>{cat.capitalize()}:</b> {count} reviews ({perc:.1f}%)", bullet_style))
    story.append(Spacer(1, 25))
    
    # 4. Visual Insights
    story.append(Paragraph("Visual Insights", section_style))
    
    # Prepare chart images
    pie_buf = io.BytesIO()
    pie_fig.savefig(pie_buf, format='png', bbox_inches='tight', dpi=200)
    pie_buf.seek(0)
    
    bar_buf = io.BytesIO()
    bar_fig.savefig(bar_buf, format='png', bbox_inches='tight', dpi=200)
    bar_buf.seek(0)
    
    # Reduced slightly for better fit within margins
    img_pie = Image(pie_buf, width=3.1*inch, height=3.1*inch)
    img_bar = Image(bar_buf, width=3.1*inch, height=3.1*inch)
    
    chart_table = Table([[img_pie, img_bar]], colWidths=[3.25*inch, 3.25*inch])
    chart_table.setStyle(TableStyle([
        ('ALIGN', (0,0), (-1,-1), 'CENTER'),
        ('VALIGN', (0,0), (-1,-1), 'MIDDLE'),
        ('LEFTPADDING', (0,0), (-1,-1), 0),
        ('RIGHTPADDING', (0,0), (-1,-1), 0),
    ]))
    story.append(chart_table)
    story.append(Spacer(1, 25))
    
    # 5. Detailed Data Table
    story.append(PageBreak())
    story.append(Paragraph("Detailed Data Table", section_style))
    
    data = [["#", "Review Text", "Sentiment"]]
    for i, (_, row) in enumerate(df.iterrows()):
        txt = str(row['review_text'])
        sent = str(row['predicted_sentiment']).capitalize()
        data.append([str(i+1), Paragraph(txt, styles['Normal']), sent])
    
    # Professional Table Styling
    t = Table(data, colWidths=[0.5*inch, 5.0*inch, 1.25*inch], repeatRows=1)
    t.setStyle(TableStyle([
        # Header Styling
        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor("#34495E")),
        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
        ('FONTSIZE', (0, 0), (-1, 0), 11),
        
        # Row Styling
        ('FONTNAME', (0, 1), (-1, -1), 'Helvetica'),
        ('FONTSIZE', (0, 1), (-1, -1), 10),
        ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.HexColor("#F2F4F4")]),
        
        # Alignment & Padding
        ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
        ('ALIGN', (0, 0), (0, -1), 'CENTER'),
        ('ALIGN', (-1, 0), (-1, -1), 'CENTER'),
        ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
        ('TOPPADDING', (0, 0), (-1, -1), 8),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 8),
        ('LEFTPADDING', (0, 0), (-1, -1), 8),
        ('RIGHTPADDING', (0, 0), (-1, -1), 8),
        
        # Grid
        ('LINEBELOW', (0, 0), (-1, 0), 2, colors.HexColor("#2C3E50")),
        ('GRID', (0, 1), (-1, -1), 0.5, colors.lightgrey),
    ]))
    story.append(t)
    
    # Build PDF
    reporter = PDFReport(product_name)
    doc.build(story, onFirstPage=reporter.add_page_number, onLaterPages=reporter.add_page_number)
    
    pdf_val = buffer.getvalue()
    buffer.close()
    return pdf_val

@st.cache_resource(show_spinner=False)
def initialize_nltk():
    """
    Optimize NLTK usage by downloading resources only once
    and returning necessary objects to avoid re-initialization.
    """
    resources = [
        ('punkt', 'tokenizers/punkt'),
        ('punkt_tab', 'tokenizers/punkt_tab'),
        ('stopwords', 'corpora/stopwords'),
        ('wordnet', 'corpora/wordnet'),
        ('omw-1.4', 'corpora/omw-1.4')
    ]
    
    for resource, path in resources:
        try:
            nltk.data.find(path)
        except LookupError:
            nltk.download(resource, quiet=True)
    
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize
    from nltk.stem import WordNetLemmatizer
    
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    
    return word_tokenize, stop_words, lemmatizer

# Initialize NLTK resources once
word_tokenize, stop_words, lemmatizer = initialize_nltk()

@st.cache_resource(show_spinner=False)
def load_models():
    """
    Load ML models and vectorizers only once and cache them in memory.
    This prevents reloading large files on every interaction.
    """
    try:
        # Load Vectorizer
        vectorizer_path = os.path.join(MODELS_DIR, 'tfidf_vectorizer.pkl')
        with open(vectorizer_path, 'rb') as f:
            vectorizer = pickle.load(f)
            
        # Load Model
        model_path = os.path.join(MODELS_DIR, 'sentiment_classifier_nb.pkl')
        with open(model_path, 'rb') as f:
            model = pickle.load(f)
            
        return vectorizer, model
    except FileNotFoundError:
        return None, None

# Load models with a spinner only on the first run
with st.spinner("üöÄ Booting up AI Engine..."):
    vectorizer, model = load_models()

if vectorizer is None or model is None:
    st.error("‚ùå Model files not found! Please ensure Step 3 and Step 4 are completed.")
    st.stop()

# ============================================================================
# PERFORMANCE OPTIMIZATION: 3. CACHED PREPROCESSING
# ============================================================================

@st.cache_data(show_spinner=False)
def preprocess_text(text):
    """
    Cache the preprocessing results.
    """
    if not isinstance(text, str) or not text.strip():
        return ""
        
    # 1. Lowercasing
    text = text.lower()
    
    # 2. Remove punctuation, special characters, and numbers
    text = re.sub(r'[^\w\s]|[\d]', '', text)
    
    # 3. Tokenization
    tokens = word_tokenize(text)
    
    # 4. Stopwords Removal & Lemmatization
    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words]
    
    return ' '.join(tokens)

# ============================================================================
# USER INTERFACE - SHARED COMPONENTS
# ============================================================================

# Header
st.title("üõçÔ∏è Sentiment Analysis System")
st.markdown("Analyze product reviews and detect sentiment instantly using AI.")
st.markdown("---")

# Shared Product Information Section
with st.container():
    st.subheader("üì¶ Product Information")
    col1, col2 = st.columns([2, 1])

    with col1:
        product_name = st.text_input(
            "Product Name (Optional)",
            placeholder="e.g., Samsung Galaxy M14",
            help="This name will appear in generated reports."
        )

    with col2:
        rating = st.selectbox(
            "User Rating (Optional)",
            options=[0, 1, 2, 3, 4, 5],
            index=5,
            format_func=lambda x: f"{x} Stars" if x > 0 else "Not Rated",
            help="Used for comparison with AI predictions."
        )

st.markdown("---")

# Analysis Mode Selector
analysis_mode = st.radio(
    "Select Analysis Mode",
    options=["Single Review", "Bulk Review Upload"],
    horizontal=True,
    help="Choose between individual review analysis or batch CSV processing."
)

st.markdown("---")

# ============================================================================
# SINGLE REVIEW MODE
# ============================================================================
if analysis_mode == "Single Review":
    review_type = st.radio(
        "Review Format",
        options=["Text Review", "Audio (Coming Soon)", "Video (Coming Soon)"],
        index=0,
        horizontal=True
    )

    if review_type != "Text Review":
        st.info("‚ö†Ô∏è Feature under development. Defaulting to Text mode.")

    review_text = st.text_area(
        "Review Content",
        height=150,
        placeholder="Type your detailed product review here..."
    )

    analyze_btn = st.button("üîç Analyze Sentiment", type="primary", use_container_width=True)

    if analyze_btn:
        if not review_text.strip():
            st.warning("‚ö†Ô∏è Please provide some review text.")
        else:
            with st.spinner("Analyzing..."):
                processed_text = preprocess_text(review_text)
                features = vectorizer.transform([processed_text])
                probabilities = model.predict_proba(features)[0]
                
                prediction_idx = np.argmax(probabilities)
                prediction_label = model.classes_[prediction_idx]
                confidence = probabilities[prediction_idx] * 100
                
                sentiment_map = {
                    'positive': {'color': 'green', 'emoji': 'üòÉ'},
                    'neutral': {'color': 'orange', 'emoji': 'üòê'},
                    'negative': {'color': 'red', 'emoji': 'üòî'}
                }
                
                result = sentiment_map.get(prediction_label, {'color': 'gray', 'emoji': '‚ùì'})
                
                # Display Results
                st.markdown(f"### Results for: {product_name if product_name else 'Unnamed Product'}")
                
                st.markdown(
                    f"""
                    <div style="background-color: #f0f2f6; padding: 20px; border-radius: 10px; border-left: 5px solid {result['color']}; text-align: center;">
                        <h2 style="color: {result['color']}; margin:0;">{result['emoji']} {prediction_label.capitalize()}</h2>
                        <p style="margin:5px; font-size: 18px;">Confidence: <strong>{confidence:.2f}%</strong></p>
                    </div>
                    """,
                    unsafe_allow_html=True
                )
                
                if rating > 0:
                    is_match = (
                        (rating >= 4 and prediction_label == 'positive') or 
                        (rating == 3 and prediction_label == 'neutral') or 
                        (rating <= 2 and prediction_label == 'negative')
                    )
                    if is_match:
                        st.success("‚úÖ Prediction alignment: Matches user star rating!")
                    else:
                        st.info("‚ÑπÔ∏è Note: Detected sentiment differs from star rating.")

                with st.expander("üìä Probability Breakdown"):
                    probs_df = pd.DataFrame({
                        'Sentiment': [c.capitalize() for c in model.classes_],
                        'Probability': probabilities
                    })
                    st.bar_chart(probs_df.set_index('Sentiment'))

# ============================================================================
# BULK REVIEW MODE
# ============================================================================
else:
    st.header("üìÇ Bulk Review Upload")
    st.info("üí° **Instructions:** Ensure your CSV contains a column named **'review_text'**.")
    
    # Sample Template
    sample_data = pd.DataFrame({"review_text": ["Great product!", "Bad quality.", "It's average.", "Cool but expensive.", "Fast delivery!"]})
    sample_csv = sample_data.to_csv(index=False).encode('utf-8')
    st.download_button("üì• Download Sample CSV", sample_csv, "sample_bulk_reviews.csv", "text/csv")
    
    st.markdown("---")
    uploaded_file = st.file_uploader("Upload CSV File", type=["csv"])
    
    if uploaded_file:
        try:
            df_bulk = pd.read_csv(uploaded_file)
            if "review_text" not in df_bulk.columns:
                st.error("‚ùå Column 'review_text' missing.")
            else:
                st.write(f"‚úÖ Found {len(df_bulk)} reviews to process.")
                
                if st.button("üöÄ Analyze All Reviews", type="primary", use_container_width=True):
                    start_time = time.time()
                    
                    with st.spinner("Analyzing bulk reviews..."):
                        # PERFORMANCE OPTIMIZATION: 
                        # 1. Preprocessing is cached per text
                        df_bulk['processed_text'] = df_bulk['review_text'].apply(preprocess_text)
                        
                        # 2. Vectorized Feature Extraction (Single batch call)
                        features = vectorizer.transform(df_bulk['processed_text'])
                        
                        # 3. Vectorized Prediction (Single batch call)
                        predictions = model.predict(features)
                        df_bulk['predicted_sentiment'] = predictions
                        
                    duration = time.time() - start_time
                    st.success(f"‚ö° Extraction & Analysis completed in {duration:.2f} seconds!")
                    
                    # Dashboard Summary
                    if product_name:
                        st.subheader(f"üìä Summary for: {product_name}")
                    
                    counts = df_bulk['predicted_sentiment'].value_counts()
                    for cat in ['positive', 'neutral', 'negative']:
                        if cat not in counts: counts[cat] = 0
                        
                    m1, m2, m3 = st.columns(3)
                    m1.metric("üòä Positive", counts['positive'])
                    m2.metric("üòê Neutral", counts['neutral'])
                    m3.metric("üòî Negative", counts['negative'])
                    
                    # Visualizations
                    dist_data = counts.reindex(['positive', 'neutral', 'negative']).fillna(0)
                    col_v1, col_v2 = st.columns(2)
                    
                    with col_v1:
                        fig_pie, ax_pie = plt.subplots(figsize=(6, 6))
                        ax_pie.pie(dist_data, labels=[c.capitalize() for c in dist_data.index], autopct='%1.1f%%', startangle=140, colors=['#28a745', '#ffc107', '#dc3545'])
                        ax_pie.set_title("Sentiment Proportions")
                        st.pyplot(fig_pie)
                        
                    with col_v2:
                        # For PDF, we need a Matplotlib bar chart too
                        fig_bar, ax_bar = plt.subplots(figsize=(6, 6))
                        dist_data.plot(kind='bar', color=['#28a745', '#ffc107', '#dc3545'], ax=ax_bar)
                        ax_bar.set_title("Sentiment Comparison")
                        ax_bar.set_ylabel("Review Count")
                        ax_bar.set_xticklabels([c.capitalize() for c in dist_data.index], rotation=0)
                        st.pyplot(fig_bar)
                    
                    # Table & Export
                    st.markdown("---")
                    st.subheader("üìù Processed Data Table")
                    st.dataframe(df_bulk[['review_text', 'predicted_sentiment']], use_container_width=True)
                    
                    # Export Buttons
                    c1, c2 = st.columns(2)
                    
                    with c1:
                        csv_export = df_bulk[['review_text', 'predicted_sentiment']].to_csv(index=False).encode('utf-8')
                        st.download_button("üì• Download Results (CSV)", csv_export, "bulk_results.csv", "text/csv", use_container_width=True)
                    
                    with c2:
                        with st.spinner("Generating PDF Report..."):
                            pdf_data = generate_pdf_report(df_bulk, product_name, counts, fig_pie, fig_bar)
                            
                            # Clean filename
                            clean_name = product_name.replace(" ", "_").lower() if product_name else "bulk_reviews"
                            
                            st.download_button(
                                label="üìÑ Download Full Report (PDF)",
                                data=bytes(pdf_data),
                                file_name=f"sentiment_report_{clean_name}.pdf",
                                mime="application/pdf",
                                use_container_width=True
                            )
                    
        except Exception as e:
            st.error(f"‚ùå Error: {str(e)}")

st.markdown("---")
st.caption("Sentiment Analysis System | Performance Optimized v2.0")
